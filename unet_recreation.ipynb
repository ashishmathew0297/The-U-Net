{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSA:\n",
    "\n",
    "This notebook is going to have a laid-back tone so as to ensure that whoever goes through this not only understands what is happening, but also knows of my personal thoughts on the paper as I read through it. A good way to think of this is me giving a semi-live commentary on the paper as I read through it while coding it up. I will not be covering every single detail since it would lead to too much distraction, but do expect a few out-of-context comments here and there. \n",
    "\n",
    "# U-Nets\n",
    "\n",
    "U-Nets are one of the first few segmentation algorithms, after the fully convolutional networks; thet pioneered this particular subdomain of image segmentation. Initially created for biomedical image segmentation, this algorithm is a staple in computer vision and has found many uses over the years ranging from autonomous vehicles and (the obvious) biomedical segmentation, to diffusion frameworks like Dall-E and Midjourney.\n",
    "\n",
    "This model makes use of the **encoder-decoder** architecture alongside **residual connections**, leading to an impressive level of clarity in the segmentation maps. I really want to see this in action here which is why I will be doing what I usually do with deep learning networks, that is, breaking it open to see how it functions between layers.\n",
    "\n",
    "In this notebook I will be looking at how this model works following the paper as I go along with the code. I will be looking at segmenting images, and down the line, will eventually explore the generative properties of this model in another project. Going by the theme of the original paper, I will be experimenting with the [DRIVE](https://drive.grand-challenge.org/DRIVE/) dataset, which is openly accessible (you will need to sign up in the link above to be able to download the dataset).\n",
    "\n",
    "Unzip the training and test datasets from the link and add them into the `/data` folder for this code to work.\n",
    "\n",
    "Now, let's finally get to it! We load up the packages we will need for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will also be trying to implement CUDA to hasten the training process. This is easier done on Linux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the paper by [Ronneberger et al. (2015)](https://arxiv.org/pdf/1505.04597) we are aware that historically CNNs have been good at classification tasks. However, in uses cases like biomedical imaging we might need to:\n",
    "- Localize parts our image\n",
    "- Take into account the fact that we might not have a huge dataset to work with.\n",
    "\n",
    "The U-Net builds upon the fully convolutional network to ensure that it can work with a very small dataset and yield precise segmentations.\n",
    "\n",
    "**Personal Note:** This paper has a lot of exposition describing efforts by researchers to get to this point. I personally really like this approach, and it seems to put things in a very easy-to-follow manner. I will, however be avoiding minutiae from this point forward unless it is relevant to our use case.\n",
    "\n",
    "## U-Net Architecture\n",
    "\n",
    "The most common diagram of the U-Net is taken directly from the paper as below\n",
    "\n",
    "![image.png](images/image.png)\n",
    "\n",
    "Here I notice a few main patters that will help code up this model.\n",
    "- The first pattern is:\n",
    "  $$(2 \\times (\\text{conv} + \\text{RELU}) + \\text{Maxpool}) \\xRightarrow{out(resize)}$$\n",
    "  - This block is repeated **four** times to bring the original image from its original dimensions to the lower dimensional representations.\n",
    "  - The convolution operations double the number of input channels while gradually decreasing the input dimensions.\n",
    "  - At each Maxpool, convolution output dimensions get halved. In every set of the convolution blocks except the first one, the number of channels gets doubled.\n",
    "  - This is the first part of the U or the \"encoder\".\n",
    "\n",
    "- Then we have:\n",
    "  $$2 \\times (\\text{conv} + \\text{RELU})$$\n",
    "  - A repetition of the same convolution as before is performed on the lower dimensional representation, bringing down the dimensions even further while doubling the number of channels.\n",
    "  - This can be seen as the bottom part of the U or the \"latent space\".\n",
    "\n",
    "- The third pattern is as follows:\n",
    "  $$\\xRightarrow{(resize)in}(\\text{transpose conv} + (2 \\times \\text{conv}))$$\n",
    "  - This pattern doubles the dimensions of the input channels with the help of the transpose convolution operation.\n",
    "  - The following convolution operations halve the number of channels while also decreasing the dimensions.\n",
    "  - This block is repeated **four** times.\n",
    "  - This represents the last part of the U or the \"decoder\".\n",
    "\n",
    "- The final output block is a 1-D convolution performed on its inputs to give us a segmentation map.\n",
    "\n",
    "Throughout the above, skip connections are used to give context to the layer pairs between the encoder input and the decoder output. Resizing is implemented here to make sure the inputs comply with the given layer's expected dimensions.\n",
    "\n",
    "## Loading and Studying the Dataset\n",
    "\n",
    "We will now load and study what we are working with. When loading the dataset, since it is not in the form of an h5 file, it might be a bit more hands-on to get working.\n",
    "\n",
    "Any images we get as an input will need to be resized to fit the specification given by the original paper $(572,572)$\n",
    "\n",
    "<!-- ModuleList -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eyeballDataset:\n",
    "    def __init__(self, image_path:Path, mask_path:Path):\n",
    "\n",
    "        self.images = {p.stem: p for p in image_path.iterdir()}\n",
    "\n",
    "        # [:-5] gets rid of the \"_mask\" in the key values\n",
    "        self.masks = {p.stem[:-5]: p for p in mask_path.iterdir()}\n",
    "        \n",
    "        self.ids = list(self.images.keys())\n",
    "\n",
    "        # Resize the image according to the paper and  \n",
    "        self.transforms = {\n",
    "            transforms.Resize(572),\n",
    "            transforms.ToTensor()\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # Loads up the image and mask according to the ID provided\n",
    "    def __getitem__(self, idx):\n",
    "        id_ = self.ids[idx]\n",
    "        image = Image.open(self.images[id_])\n",
    "        mask = Image.open(self.masks[id_])\n",
    "        image = self.transforms(image)\n",
    "        mask = self.transforms(mask)\n",
    "\n",
    "        return image,mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load our data and the related masks to see what we are dealing with. This is also a way to ensure that we have implemented the dataloader properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = eyeballDataset(\"data/training/images\",\"data/training/mask/\")\n",
    "test_dataset = eyeballDataset(\"data/test/mask/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
